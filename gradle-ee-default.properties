###
### DO NOT MODIFY THIS FILE in your workspace project.
###
### Instead copy the project property into 'gradle.properties' and set your
### custom value there. Or use init script / command line where appropriate.
###
### This file defines the configuration properties used by Liferay Workspace EE and
### their default values. You can check how the properties are being read by the build
### in script /gradle/liferay-workspace-ee/liferay-workspace-ee.gradle.w
###

###
### Properties which should be specified _in Gradle init script_ and passed to Gradle
### when running the build
###

# Please check file 'sample-ee-init-script.gradle' for complete list of supported
# properties and guidelines how to pass them to your Gradle build.

# Note that although you can specify all of the init-script properties on command line
# as well (-P...), it's strongly discouraged. Many of the properties in init script
# contain sensitive values (security credentials to other systems) and since the commands
# will show up in list of running processes to all other users in your OS and also
# stay in your e.g. Bash history, it would be a severe security hole.


###
### Properties which should be specified _on Gradle command line_ when running the build
###

# With what "build number" should the Liferay workspace EE build work? This is
# typically a counter managed outside of source code, in running system like Jenkins.
# Gradle will use this number to name
# You will typically want to define build of workspace using Jenkins as:
#
#   ./gradlew distBundleZip distBundleTar distBundleDeb distBundleRpm
#           -Pliferay.workspace.environment=dev
#			-PreleaseNumber=$BUILD_NUMBER
#           --no-daemon
#
# BUILD_NUMBER is a standard variable populated by Jenkins inside every job, which
# then can be used in definition of build steps.
#
releaseNumber=1


###
### Properties which should go into _gradle.properties_, if custom value is needed
###

# The type of Liferay bundle you are using, referring to app server being used.
# Only 'tomcat' is currently supported.
#
# This has to be matching the bundle that is specified in 'liferay.workspace.bundle.url'
# inside gradle.properties, which should default to Tomcat bundle URL like:
#   https://sourceforge.net/projects/lportal/files/Liferay Portal/7.0.0 GA1/liferay-portal-tomcat-7.0-ce-ga1-20160331161017956.zip
#
liferay.workspace.ee.bundle.type=tomcat
liferay.workspace.ee.bundle.app.server.directory.name=tomcat-8.0.32


##
## Jenkins management
##

# The URL where your project's Jenkins server could be reached by the project's
# build scripts.
#
# Credentials should always be passed on command line, see 'liferay.workspace.ee.jenkinsServerSecure'.
#
liferay.workspace.ee.jenkins.server.url=http://localhost:18080

# Configure whether the Jenkins server running on URL 'liferay.workspace.ee.jenkins.server.url'
# requires credentials (username + password) or not. If set to 'true', you need to provide
# 'jenkinsUserName' and 'jenkinsPassword', either one by one on command line (gradlew -P...)
# or through init script:
#   * 'jenkinsUserName' = the login to pass to Jenkins REST API
#   * 'jenkinsPassword'	= the password (or token) to use to login to the Jenkins REST API
#
# NOTE: Some actions, like restarting Jenkins remotely (may be triggered by the scripts
#       if new Jenkins plugins were installed) always require credentials with admin
#       privileges in Jenkins. This is true even if using various "X users can do anything"
#       settings in Jenkins security.
#
# WARNING: Use 'false' only for local testing and never in actual project's CI server.
#
liferay.workspace.ee.jenkins.server.secure=true

# The directory, relative to this file, where jobs and views from remote Jenkins server will
# be stored when performing backup. Also these jobs and views will be pushed to Jenkins when
# performing Jenkins restore. Initial set of jobs will be provided if this directory
# does not exist.
#
liferay.workspace.ee.jenkins.items.dir=jenkins

# Comma-separated list of environment names which will be excluded when initializing jobs
# in remote Jenkins server. All other directories in '/configs' (except special 'common')
# will have their Jenkins jobs created.
#
liferay.workspace.ee.jenkins.initial.jobs.exclude.environments=local

# Comma-separated list of Jenkins job names which will be added to the list of initial
# set of jobs defined by this workspace project. Workspace will be able to work with
# (backup & restore) only jobs defined by this workspace -- any other job, created
# in Jenkins server outside of workspace, will not be manageable from workspace.
#
# With jobs added into this property, the named jobs will be looked up in Jenkins
# server and their XMLs will be retrieved and stored when 'dumpRemoteJenkinsItems'
# task is invoked.
#
liferay.workspace.ee.jenkins.initial.jobs.extra.dumped.job.names=

##
## Configuration for baking new AMIs in AWS for every new build (e.g. 'gradlew distBundleAmi')
##

# The target region where this project AMIs will be created and stored. The temporary
# EC2 instance used to create the AMI contents will be also launched in this region.
#
# Pricing differs between regions, see https://aws.amazon.com/ec2/pricing/.
#
liferay.workspace.ee.aws.ami.primary.region=us-east-1

# The ID of the AMI on which to base the project builds. Make sure that the region
# is the same as the target AMI region ('liferay.workspace.ee.aws.ami.primary.region'),
# otherwise the base AMI won't be found and the project's AMI won't not be created.
#
# To lookup latest Ubuntu AMIs, use e.g. https://cloud-images.ubuntu.com/locator/ec2/
#   * ami-5aa69030 ~ Ubuntu 14.04 LTS, amd64, hvm:ebs-ssd (us-east-1)
#   * ami-766d771a ~ Ubuntu 14.04 LTS, amd64, hvm:ebs-ssd (eu-central-1)
#
# To lookup latest CentOS images, use e.g. https://wiki.centos.org/Cloud/AWS
#	* ami-57cd8732 ~ CentOS 6 (x86_64) - with Updates HVM (us-east-1)
#	* ami-2a868b37 ~ CentOS 6 (x86_64) - with Updates HVM (eu-central-1)
#
liferay.workspace.ee.aws.ami.base.ami.id=ami-5aa69030

# The packaging format in the Linux OS installed in the used base AMI (see 'aws.ami.base.ami.id').
# It has to be either 'deb' or 'rpm' and determines which packaging mechanism will
# be used to install Liferay bundle into the base AMI.
#   * 'deb' ~ Debian, Ubuntu etc.
#   * 'rpm' ~ RedHat based, CentOS, Fedora etc.
#
# Having this information upfront, before the EC2 instance is startes from the base AMI,
# allows us to build & upload only the respective archive to the AWS instance, therefore
# making the build of new AMI much faster.
#
liferay.workspace.ee.aws.ami.base.ami.linux.packages.format=deb

# The OS user which is set up for SSH access using the default AWS key-pair
# when new EC2 instance is created base on the base AMI.
#
# "For Amazon Linux, the user name is ec2-user. For RHEL5, the user name is either root or ec2-user.
# For Ubuntu, the user name is ubuntu. For Fedora, the user name is either fedora or ec2-user.
# For SUSE Linux, the user name is either root or ec2-user. Otherwise, if ec2-user and root don't work,
# check with your AMI provider."
#   * source: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html
#
liferay.workspace.ee.aws.ami.base.ami.ssh.user.name=ubuntu

# The type of EC2 instance to use when building the AMI with Liferay Bundle installed.
# The stronger the instance, the faster the AMI will be build, but the more it will
# cost you. However, since the EC2 instance is only needed to install DEB package, therefore
# not requiring too much CPU / RAM, it's usually enough to choose even tiny instance type,
# like t2.micro.
#
# The type should be in accordance to the 'base.ami.id' you are using -- not all
# instance types support all virtualization types avaialable in AWS:
#   * t1, m1, c1, m2 (= older generations)
#       * supports only PV virtualization, so you need to choose PV-based AMI
#           if you want to use one of thse these
#   * other (= newer generations)
#       * supports only HVM virtualization
#
liferay.workspace.ee.aws.ami.build.ec2.instance.type=t2.micro

# If you want the temporary EC2 instance for building AMI launched in non-default VPC,
# provide both 'vpc.id' and 'subnet.id'. Note that your account may not have default VPC
# at all, or only in some of the regions, see these resources relating to default VPCs
# to see if your AWS account supports / has them:
#   * http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html#detecting-platform
#	* http://serverfault.com/a/523484
#
# If you have default VPC available, you can leave these two properties empty.
#
liferay.workspace.ee.aws.ami.build.ec2.vpc.id=
liferay.workspace.ee.aws.ami.build.ec2.subnet.id=


# The String which is put into the create AMI's name to allows AWS to 'detect'
# the Platform metadata value. This is useful since AWS console will suggest
# the SSH username to use to connect to the EC2 instances (created from this AMI)
# when the 'Connect' dialog is opened.
#
# Known keys (and their likely detected 'Platform's) are:
#   Key                 | Platform
#   'ubuntu'            -> Ubuntu
#   'amzn'              -> Amazon Linux
#   'centos'            -> Cent OS
#   'redhat' / 'RHEL'   -> Red Hat
#
liferay.workspace.ee.aws.ami.platform.key=ubuntu

# Comma-separated list of AWS regions, where the built project AMIs will be copied into,
# after the primary AMI was successfully built in the primary region (see
# 'awsAmiPrimaryRegion' above).
#
# You can also copy created AMIs into other regions manually, using AWS console:
#   * https://aws.amazon.com/blogs/aws/ec2-ami-copy-between-regions/
#
liferay.workspace.ee.aws.ami.copy.to.regions=
#liferay.workspace.ee.aws.ami.copy.to.regions=us-west-1,us-east-1


##
## Configuration for building Dockerfiles (e.g. 'gradlew distBundleDockerfile')
##

# The first part of the Docker repository name to be used in when tagging built
# Docker images. The second part is added automatically based on the project's name.
#
# Example: if you specify value 'acme' and your Liferay Workspace is named 'liferay-portal'
# (setting.gradle in the Gradle project) ), the Docker repository will be named
# 'acme/liferay-portal'
#
liferay.workspace.ee.docker.repository.company=acme

# The value of the MAINTAINER directive put into Dockerfile. It should contain
# a valid name and email of the maintainer, in the form 'name <email>'.
#
# No validation or transformation is performed on the value.
#
liferay.workspace.ee.docker.maintainer=Docker Maintainer <docker-maintainer@yourdomain.com>


# The value of the EXPOSE directive put into Dockerfile, determining which ports
# will the Docker container open to the outer world. Based on Docker documentation,
# this is space-separated list of port numbers.
#
# No validation or transformation is performed on the value.
#
# This has to be in sync with your your app server configuration in '/configs/**'
# (e.g. Tomcat) -- you will want to open all used HTTP / AJP ports, as well as
# for example JMX (typically on 9000) if you want to use if from outside of the
# Docker container.
#
liferay.workspace.ee.docker.expose=8080 8081